{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL3jL_ZJ0cJN"
      },
      "outputs": [],
      "source": [
        "#@title core\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import sys\n",
        "repo_path = '/Users/szymonlukiewicz/StudioProjects/psych_gen_app/content'\n",
        "\n",
        "def setup_stylegan():\n",
        "    \"\"\"\n",
        "    Sets up the StyleGAN environment by installing required packages, cloning the necessary GitHub repository,\n",
        "    mounting Google Drive (if in Colab), downloading additional files, and loading the StyleGAN model.\n",
        "\n",
        "    Returns:\n",
        "        G (torch.nn.Module): The StyleGAN generator model.\n",
        "        face_w (torch.Tensor): A tensor of sample latent vectors.\n",
        "    \"\"\"\n",
        "    global repo_path\n",
        "    # Change directory to content\n",
        "    os.chdir(repo_path+'')\n",
        "\n",
        "    # Download additional files\n",
        "    os.system('gdown 1O79M5F5G3ktmt1-zeccbJf1Bhe8K9ROz')\n",
        "    if not os.path.exists(repo_path+'/omi'):\n",
        "        os.system('git clone https://github.com/jcpeterson/omi')\n",
        "        os.system('unzip content/omi/attribute_ratings.zip')\n",
        "\n",
        "    # Add necessary paths to sys.path\n",
        "    sys.path.append(repo_path+'/psychGAN/stylegan3')\n",
        "    sys.path.append(repo_path+'/psychGAN')\n",
        "    os.chdir(repo_path+'/psychGAN')\n",
        "\n",
        "    # Download the StyleGAN model file if not present\n",
        "    model_path = repo_path+\"/psychGAN/stylegan2-ffhq-1024x1024.pkl\"\n",
        "    if not os.path.exists(model_path):\n",
        "    #   !rm content/psychGAN/stylegan2-ffhq-1024x1024.pkl\n",
        "      !wget https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-1024x1024.pkl\n",
        "\n",
        "    # Load the StyleGAN model\n",
        "    device = torch.device('mps')\n",
        "    with open(model_path, 'rb') as fp:\n",
        "        G = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "    # Compute the average latent vector\n",
        "    all_z = torch.randn([1, G.mapping.z_dim], device=device)\n",
        "    face_w = G.mapping(all_z, None, truncation_psi=0.5)\n",
        "\n",
        "    return G, face_w, device\n",
        "G, face_w, device = setup_stylegan()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKHGRicy1xC4"
      },
      "outputs": [],
      "source": [
        "#@title utils\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import IPython.display\n",
        "from PIL import Image, ImageDraw\n",
        "from math import ceil\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "import torch\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "def listify(x):\n",
        "    \"\"\"\n",
        "    Converts a single element or a pandas DataFrame/Series to a list.\n",
        "    If the input is already a list, it returns the input unmodified.\n",
        "\n",
        "    Args:\n",
        "        x: The input to be listified.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of the input elements.\n",
        "    \"\"\"\n",
        "    if isinstance(x, (list, pd.DataFrame, pd.Series)):\n",
        "        return list(x)\n",
        "    return [x]\n",
        "\n",
        "def display_image(image_array, format='png', jpeg_fallback=True):\n",
        "    \"\"\"\n",
        "    Displays an image in IPython.\n",
        "\n",
        "    Args:\n",
        "        image_array: A numpy array representing the image.\n",
        "        format: The format of the image to display.\n",
        "        jpeg_fallback: Whether to fall back to JPEG if the image is too large.\n",
        "\n",
        "    Returns:\n",
        "        The IPython.display object.\n",
        "    \"\"\"\n",
        "    image_array = np.asarray(image_array, dtype=np.uint8)\n",
        "    str_file = BytesIO()\n",
        "    PIL.Image.fromarray(image_array).save(str_file, format)\n",
        "    im_data = str_file.getvalue()\n",
        "    try:\n",
        "        return IPython.display.display(IPython.display.Image(im_data))\n",
        "    except IOError as e:\n",
        "        if jpeg_fallback and format != 'jpeg':\n",
        "            print(f'Warning: image was too large to display in format \"{format}\"; trying jpeg instead.')\n",
        "            return display_image(image_array, format='jpeg')\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def create_image_grid(images, scale=1, rows=1):\n",
        "    \"\"\"\n",
        "    Creates a grid of images.\n",
        "\n",
        "    Args:\n",
        "        images: A list of PIL.Image objects.\n",
        "        scale: The scale factor for each image.\n",
        "        rows: The number of rows in the grid.\n",
        "\n",
        "    Returns:\n",
        "        A single PIL.Image object containing the grid of images.\n",
        "    \"\"\"\n",
        "    w, h = images[0].size\n",
        "    w, h = int(w * scale), int(h * scale)\n",
        "    height = rows * h\n",
        "    cols = ceil(len(images) / rows)\n",
        "    width = cols * w\n",
        "    canvas = PIL.Image.new('RGBA', (width, height), 'white')\n",
        "    for i, img in enumerate(images):\n",
        "        img = img.resize((w, h), PIL.Image.ANTIALIAS)\n",
        "        canvas.paste(img, (w * (i % cols), h * (i // cols)))\n",
        "    return canvas\n",
        "\n",
        "def dot_product(x, y):\n",
        "    \"\"\"\n",
        "    Computes the normalized dot product of two vectors.\n",
        "\n",
        "    Args:\n",
        "        x, y: The vectors to compute the dot product of. Can be file paths or numpy arrays.\n",
        "\n",
        "    Returns:\n",
        "        The normalized dot product of x and y.\n",
        "    \"\"\"\n",
        "    x = np.load(x) if isinstance(x, str) else x\n",
        "    y = np.load(y) if isinstance(y, str) else y\n",
        "    x_norm = x[1] if len(x.shape) > 1 else x\n",
        "    y_norm = y[1] if len(y.shape) > 1 else y\n",
        "    return np.dot(x_norm / np.linalg.norm(x_norm), y_norm / np.linalg.norm(y_norm))\n",
        "\n",
        "def read(target, passthrough=True):\n",
        "    \"\"\"\n",
        "    Transforms a path or array of coordinates into a standard format.\n",
        "\n",
        "    Args:\n",
        "        target: A path to the coordinate file or a numpy array.\n",
        "        passthrough: If True, returns the target if it cannot be transformed.\n",
        "\n",
        "    Returns:\n",
        "        Transformed target or original target based on passthrough.\n",
        "    \"\"\"\n",
        "    if target is None:\n",
        "        return 0\n",
        "    if isinstance(target, PIL.Image.Image):\n",
        "        return None\n",
        "    if isinstance(target, str):\n",
        "        try:\n",
        "            target = np.load(target)\n",
        "        except:\n",
        "            return target if passthrough else None\n",
        "    if list(target.shape) == [1, 18, 512] or target.shape[0] == 18 or passthrough:\n",
        "        return target\n",
        "    if target.shape[0] in [1, 512]:\n",
        "        return np.tile(target, (18, 1)) if isinstance(target, np.ndarray) else torch.tile(target, (18, 1))\n",
        "    return target\n",
        "\n",
        "def show_faces(target, add=None, subtract=False, plot=True, grid=True, rows=1, labels = None, device='cuda:0'):\n",
        "    \"\"\"\n",
        "    Displays or returns images of faces generated from latent vectors.\n",
        "\n",
        "    Args:\n",
        "        target: Latent vectors or paths to images. Can be a string, np.array, or list thereof.\n",
        "        add: Latent vector to add to the target. Can be None, np.array, or list thereof.\n",
        "        subtract: If True, subtracts 'add' from 'target'.\n",
        "        plot: If True, plots the images using matplotlib.\n",
        "        grid: If True, displays images in a grid.\n",
        "        rows: Number of rows in the grid.\n",
        "        device: Device for PyTorch operations.\n",
        "        G: The StyleGAN generator model.\n",
        "\n",
        "    Returns:\n",
        "        PIL images or None, depending on the 'plot' argument.\n",
        "    \"\"\"\n",
        "    transform = Compose([\n",
        "        Resize(512),\n",
        "        lambda x: torch.clamp((x + 1) / 2, min=0, max=1)\n",
        "    ])\n",
        "\n",
        "    target, add = listify(target), listify(add)\n",
        "    to_generate = [read(t, False) for t in target if read(t, False) is not None]\n",
        "\n",
        "    if add[0] is not None:\n",
        "        if len(add) == len(target):\n",
        "            to_generate_add = [t + read(a) for t, a in zip(target, add)]\n",
        "            to_generate_sub = [t - read(a) for t, a in zip(target, add)]\n",
        "        else:\n",
        "            to_generate_add = [t + read(add[0]) for t in target]\n",
        "            to_generate_sub = [t - read(add[0]) for t in target]\n",
        "        to_generate = [m for pair in zip(to_generate_sub, to_generate, to_generate_add) for m in pair] if subtract else [m for pair in zip(to_generate, to_generate_add) for m in pair]\n",
        "\n",
        "    other = [PIL.Image.open(t) for t in target if isinstance(t, str) and not '.npy' in t]\n",
        "    other += [t for t in target if isinstance(t, PIL.Image.Image)]\n",
        "    for im in target:\n",
        "        try:\n",
        "            other += [TF.to_pil_image(transform(im))]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    images_pil = []\n",
        "    if len(to_generate) > 0:\n",
        "        global G\n",
        "        with torch.no_grad():\n",
        "            face_w = torch.tensor(to_generate, device=device)\n",
        "            images = G.synthesis(face_w.view(-1, 18, 512))\n",
        "            images_pil = [TF.to_pil_image(transform(im)) for im in images]\n",
        "\n",
        "    images_pil += [(t) for t in other]\n",
        "\n",
        "    if plot:\n",
        "        display_images(images_pil, grid, rows, labels=labels)\n",
        "    else:\n",
        "        return create_image_grid(images_pil, rows=rows) if grid else images_pil\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from PIL import ImageFont\n",
        "import urllib.request\n",
        "import functools\n",
        "import io\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "\n",
        "def add_label_to_image(image, label, position=(10, 10), font_size=20):\n",
        "    \"\"\"\n",
        "    Adds a label with a black stroke to an image at the specified position.\n",
        "\n",
        "    Args:\n",
        "        image: PIL.Image object.\n",
        "        label: Text to add to the image.\n",
        "        position: Tuple specifying the position to place the text.\n",
        "        font_size: Size of the font.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image object with text added.\n",
        "    \"\"\"\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # You can use a system font or a bundled .ttf file\n",
        "    font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
        "    font = ImageFont.truetype(font_path, font_size)\n",
        "\n",
        "    # Get the bounding box for the text\n",
        "    bbox = draw.textbbox(position, label, font=font)\n",
        "    text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
        "\n",
        "    # Adjust position based on the text height\n",
        "    position = (position[0], position[1] - text_height*.5)\n",
        "\n",
        "    # Outline (stroke) parameters\n",
        "    stroke_width = 2\n",
        "    stroke_fill = \"black\"\n",
        "\n",
        "    # Draw text with outline\n",
        "    draw.text(position, label, font=font, fill=\"white\", stroke_width=stroke_width, stroke_fill=stroke_fill, textlength = text_width)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "def display_images(images, grid, rows, labels):\n",
        "    \"\"\"\n",
        "    Helper function to display images using matplotlib, with optional labels on each image.\n",
        "\n",
        "    Args:\n",
        "        images: A list of PIL.Image objects.\n",
        "        grid: If True, displays images in a grid.\n",
        "        rows: Number of rows in the grid.\n",
        "        labels: List of labels for each image; if provided, labels will be added to images.\n",
        "    \"\"\"\n",
        "    if labels:\n",
        "        images = [add_label_to_image(im.copy(), lbl) for im, lbl in zip(images, labels)]\n",
        "\n",
        "    if grid and len(images) > 1:\n",
        "        cols = (len(images) + rows - 1) // rows  # Compute number of columns needed\n",
        "        fig, axs = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
        "        axs = axs.flatten()  # Flatten the array of axes for easier iteration\n",
        "        for idx, (im, ax) in enumerate(zip(images, axs)):\n",
        "            ax.imshow(im)\n",
        "            ax.axis('off')  # Hide axes\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        for idx, im in enumerate(images):\n",
        "            plt.figure(figsize=(5, 5))\n",
        "            plt.imshow(im)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVdIgzCvpgHb"
      },
      "outputs": [],
      "source": [
        "#@title init\n",
        "%cd content/psychGAN\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import shutil\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL.Image\n",
        "from PIL import Image, ImageDraw\n",
        "import IPython.display\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "sys.path.append(repo_path+'/psychGAN/stylegan3')\n",
        "sys.path.append(repo_path+'/psychGAN')\n",
        "\n",
        "import sys\n",
        "\n",
        "import io\n",
        "import os, time\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import copy\n",
        "import imageio\n",
        "import unicodedata\n",
        "import re\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "# from google.colab import files\n",
        "from time import perf_counter\n",
        "\n",
        "from stylegan3.dnnlib.util import open_url\n",
        "df = pd.read_hdf(repo_path+'/coords_wlosses.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKTYgItggx5y"
      },
      "outputs": [],
      "source": [
        "#@title model\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.transforms import Compose, Resize\n",
        "import torchvision.transforms.functional as TF\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Model Definition\n",
        "import torch\n",
        "from torch import nn\n",
        "class MultiTargetRegressor(nn.Module):\n",
        "    def __init__(self, latent_dim, target_dim):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.1),  # Removed feature dropout\n",
        "\n",
        "            nn.Linear(1024, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(512, target_dim),\n",
        "            # Consider an output activation here if appropriate for your target data\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class EnsembleRegressor(nn.Module):\n",
        "    def __init__(self, models):\n",
        "        super().__init__()\n",
        "        self.models = nn.ModuleList(models)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [model(x) for model in self.models]\n",
        "        return torch.mean(torch.stack(outputs), dim=0)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from torchvision.transforms import Compose, Resize\n",
        "import torchvision.transforms.functional as TF\n",
        "from typing import List, Dict, Union, Tuple\n",
        "from torchdiffeq import odeint\n",
        "\n",
        "# Assume previous class definitions for VectorFieldTransformer, RatingODE, etc., are available.\n",
        "\n",
        "class StyleGANFlowBackend:\n",
        "    def __init__(self,\n",
        "                 generator: nn.Module,\n",
        "                 flow_models: Dict[str, nn.Module],\n",
        "                 regression_models: Dict[str, nn.Module],\n",
        "                 trait_stats_df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Initializes the StyleGANFlowBackend for flow-based manipulations.\n",
        "        \"\"\"\n",
        "        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"‚úÖ StyleGANFlowBackend initialized on device: {self.device}\")\n",
        "\n",
        "        self.G = generator.to(self.device).eval()\n",
        "        self.flow_models = {k: m.to(self.device).eval() for k, m in flow_models.items()}\n",
        "        self.models = {k: m.to(self.device).eval() for k, m in regression_models.items()}\n",
        "        self.dimension_names = [*self.flow_models.keys()]\n",
        "        # Handling for trait_stats_df if it's provided\n",
        "        if trait_stats_df is not None and not trait_stats_df.empty:\n",
        "            self.df = trait_stats_df\n",
        "            self.m, self.std = self.df[self.dimension_names].mean(), self.df[self.dimension_names].std()\n",
        "        else:\n",
        "            self.df = pd.DataFrame()\n",
        "\n",
        "\n",
        "    def to_w(self, target: Union[str, Path, np.ndarray, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"Converts various input types to a correctly shaped w latent tensor.\"\"\"\n",
        "        if isinstance(target, (str, Path)):\n",
        "            target = np.load(target) if \".npy\" in str(target) else torch.load(target)\n",
        "        if isinstance(target, np.ndarray):\n",
        "            target = torch.tensor(target, dtype=torch.float32, device=self.device)\n",
        "        if target.dim() < 2: target = target.reshape(1, 1, 512)\n",
        "        if target.dim() == 2: target = target.unsqueeze(0)\n",
        "        if target.shape[1] == 1: target = target.repeat(1, 18, 1)\n",
        "        return target.to(self.device).reshape(-1, 18, 512)\n",
        "\n",
        "    def _to_pil(self, images_tensor: torch.Tensor):\n",
        "        \"\"\"Post-processes and converts a tensor of images to a list of PIL Images.\"\"\"\n",
        "        transform = Compose([Resize(512), lambda x: torch.clamp((x + 1) / 2, 0, 1)])\n",
        "        return [TF.to_pil_image(img) for img in transform(images_tensor.cpu())]\n",
        "\n",
        "    def _predict_ratings(self, w_tensor: torch.Tensor) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Predicts ratings for all registered models from a w tensor.\"\"\"\n",
        "        w0 = w_tensor[:, 0, :]\n",
        "        return {label: model(w0).cpu().numpy() for label, model in self.models.items()}\n",
        "\n",
        "    def _calculate_trajectory(self,\n",
        "                              target_dim: str,\n",
        "                              max_strength: float,\n",
        "                              n_levels: int,\n",
        "                              initial_w_for_flow: torch.Tensor,\n",
        "                              max_steps: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculates a single trajectory and returns the latent codes at each level.\n",
        "        \"\"\"\n",
        "        if n_levels == 0:\n",
        "            return initial_w_for_flow.unsqueeze(0)\n",
        "\n",
        "        is_forward = max_strength >= 0\n",
        "        flow_field = self.flow_models[target_dim]\n",
        "\n",
        "        # Define the time points for the ODE solver to evaluate\n",
        "        t_span = torch.linspace(0, abs(max_strength), n_levels + 1, device=self.device)\n",
        "\n",
        "        # Define the ODE function for the solver\n",
        "        ode_func = lambda t, w: (1 if is_forward else -1) * flow_field(w)\n",
        "\n",
        "        print(f\"üöÄ Computing {'forward' if is_forward else 'backward'} trajectory for '{target_dim}'...\")\n",
        "\n",
        "        # Use a simplified solver call\n",
        "        trajectory_w0 = odeint(ode_func, initial_w_for_flow, t_span, method='rk4')\n",
        "\n",
        "        return trajectory_w0\n",
        "\n",
        "    def _generate_image_and_meta(self, w0: torch.Tensor, base_w: torch.Tensor, initial_ratings: dict, config: dict) -> Tuple:\n",
        "        \"\"\"Helper to generate an image and its metadata from a w0 vector.\"\"\"\n",
        "        manipulated_w = base_w.clone()\n",
        "        manipulated_w[:, :, :] = w0.unsqueeze(1) # Apply to all layers\n",
        "\n",
        "        if config.get('preserve_identity', True):\n",
        "            style_layers = range(config.get('latents_from', 0), config.get('latents_to', 18), 2)\n",
        "            manipulated_w[:, style_layers, :] = base_w[:, style_layers, :]\n",
        "\n",
        "        image = self._to_pil(self.G.synthesis(manipulated_w))[0]\n",
        "        final_ratings = self._predict_ratings(manipulated_w)\n",
        "        distance = torch.norm(manipulated_w[:, 0, :] - base_w[:, 0, :], dim=-1).cpu().numpy()\n",
        "\n",
        "        metadata = {\n",
        "            \"initial_ratings\": initial_ratings,\n",
        "            \"final_ratings\": final_ratings,\n",
        "            \"distance\": distance\n",
        "        }\n",
        "        return image, metadata\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, config: Dict, w: torch.Tensor = None) -> Tuple[List[List], List[List]]:\n",
        "        # --- 1. Unpack Config and Initialize ---\n",
        "        dims_config = config[\"manipulated_dimensions\"]\n",
        "        num_faces = config.get(\"num_faces\", 1)\n",
        "        truncation = config.get(\"truncation_psi\", 0.7)\n",
        "        max_steps = config.get(\"max_steps\", 100)\n",
        "\n",
        "        base_faces_w = self.to_w(w) if w is not None else self.G.mapping(torch.randn([num_faces, self.G.z_dim], device=self.device), None, truncation_psi=truncation)\n",
        "\n",
        "        all_image_grids, all_metadata_grids = [], []\n",
        "\n",
        "        # --- 2. Process each base face ---\n",
        "        for i in range(num_faces):\n",
        "            base_w = base_faces_w[i:i+1]\n",
        "            w_for_flow = base_w[:, 0, :]\n",
        "            initial_ratings = self._predict_ratings(base_w)\n",
        "\n",
        "            # --- 3. Calculate Trajectories ---\n",
        "            dim1_cfg = dims_config[0]\n",
        "            traj1_w0 = self._calculate_trajectory(dim1_cfg['name'], dim1_cfg['strength'], dim1_cfg['n_levels'], w_for_flow, max_steps)\n",
        "\n",
        "            n_levels1 = dim1_cfg['n_levels']\n",
        "\n",
        "            # --- 4. Handle 1D or 2D manipulation ---\n",
        "            if len(dims_config) == 1:\n",
        "                # --- 4a. Single Dimension (One Row) ---\n",
        "                n_levels2 = 0\n",
        "                image_grid = [[None for _ in range(n_levels1 + 1)]]\n",
        "                metadata_grid = [[None for _ in range(n_levels1 + 1)]]\n",
        "\n",
        "                for c in range(n_levels1 + 1):\n",
        "                    w0 = traj1_w0[c]\n",
        "                    img, meta = self._generate_image_and_meta(w0, base_w, initial_ratings, config)\n",
        "                    image_grid[0][c] = img\n",
        "                    meta[\"manipulation\"] = f\"{dim1_cfg['name']}_{c}\"\n",
        "                    metadata_grid[0][c] = meta\n",
        "\n",
        "            else:\n",
        "                # --- 4b. Two Dimensions (Grid) ---\n",
        "                dim2_cfg = dims_config[1]\n",
        "                traj2_w0 = self._calculate_trajectory(dim2_cfg['name'], dim2_cfg['strength'], dim2_cfg['n_levels'], w_for_flow, max_steps)\n",
        "                n_levels2 = dim2_cfg['n_levels']\n",
        "\n",
        "                image_grid = [[None for _ in range(n_levels2 + 1)] for _ in range(n_levels1 + 1)]\n",
        "                metadata_grid = [[None for _ in range(n_levels2 + 1)] for _ in range(n_levels1 + 1)]\n",
        "\n",
        "                # Fill the first column (Dimension 1)\n",
        "                for r in range(n_levels1 + 1):\n",
        "                    w0 = traj1_w0[r]\n",
        "                    img, meta = self._generate_image_and_meta(w0, base_w, initial_ratings, config)\n",
        "                    image_grid[r][0] = img\n",
        "                    meta[\"manipulation\"] = f\"{dim1_cfg['name']}_{r}\"\n",
        "                    metadata_grid[r][0] = meta\n",
        "\n",
        "                # Fill the rest of the first row (Dimension 2), starting from the second element\n",
        "                for c in range(1, n_levels2 + 1):\n",
        "                    w0 = traj2_w0[c]\n",
        "                    img, meta = self._generate_image_and_meta(w0, base_w, initial_ratings, config)\n",
        "                    image_grid[0][c] = img\n",
        "                    meta[\"manipulation\"] = f\"{dim2_cfg['name']}_{c}\"\n",
        "                    metadata_grid[0][c] = meta\n",
        "\n",
        "            all_image_grids.append(image_grid)\n",
        "            all_metadata_grids.append(metadata_grid)\n",
        "\n",
        "        return all_image_grids, all_metadata_grids\n",
        "\n",
        "if not os.path.exists(\"final_models.zip\"):\n",
        "  !gdown 1pPjOd-mx-d-vOw1QR_lpJoJmLAGdkI3W\n",
        "  !unzip final_models.zip\n",
        "if not \"models\" in dir():\n",
        "  all_labels = [col for col in df.columns if col not in ['Unnamed: 0', 'stimulus', 'loss', 'dlatents']]\n",
        "\n",
        "  models = [EnsembleRegressor([MultiTargetRegressor(512,1) for _ in range(8)]) for label in all_labels]\n",
        "  for m,l in zip(models,all_labels):\n",
        "    m.load_state_dict(torch.load(f\"final_models/ensemble_{l}.pt\", map_location='mps'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiKHyQDRVfUa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ControlGradientModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A wrapper class that computes the gradient of a control model's output\n",
        "    with respect to its input latent vector 'w'.\n",
        "\n",
        "    This allows a standard regression model (e.g., for age or gender) to act\n",
        "    as a \"flow\" model within the StyleGANFlowBackend, where the \"flow\" is\n",
        "    the direction of the gradient.\n",
        "    \"\"\"\n",
        "    def __init__(self, control_model: nn.Module):\n",
        "        \"\"\"\n",
        "        Initializes the ControlGradientModel.\n",
        "\n",
        "        Args:\n",
        "            control_model (nn.Module): A pre-trained model that takes a latent\n",
        "                                       vector `w` and outputs a scalar value\n",
        "                                       (e.g., predicted age).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = control_model\n",
        "        # Ensure the model is in evaluation mode as we only need it for inference.\n",
        "        self.model.eval()\n",
        "\n",
        "    def forward(self, w: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes and returns the gradient of the control model's output.\n",
        "\n",
        "        This method is called by the NeuralODE solver, which expects a direction\n",
        "        vector (the gradient) for each point 'w' in the latent space.\n",
        "\n",
        "        Args:\n",
        "            w (torch.Tensor): A batch of input latent vectors of shape [N, 512].\n",
        "            **kwargs: Accepts and ignores additional arguments (like 'ratings') that\n",
        "                      might be passed by the ODE wrapper for compatibility.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The computed gradient for each latent vector in the batch,\n",
        "                          representing the direction of steepest ascent for the attribute.\n",
        "        \"\"\"\n",
        "        # The gradient calculation needs to be within a torch.enable_grad() context.\n",
        "        with torch.enable_grad():\n",
        "            # Detach 'w' from any previous computation graph and enable gradient tracking.\n",
        "            w_for_grad = w.detach().clone().requires_grad_(True)\n",
        "\n",
        "            # Perform a forward pass through the control model (e.g., get age prediction).\n",
        "            output = self.model(w_for_grad)\n",
        "\n",
        "            # Sum the output to create a scalar value. Calling .backward() on this\n",
        "            # scalar computes the gradient of the sum with respect to w_for_grad.\n",
        "            # This is equivalent to computing the gradient for each item in the batch independently.\n",
        "            torch.sum(output).backward()\n",
        "\n",
        "            # The gradient is now stored in .grad attribute of the input tensor.\n",
        "            # We detach it from the graph before returning.\n",
        "            gradient = w_for_grad.grad.detach()\n",
        "\n",
        "        return gradient\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1. Create a dictionary of the new gradient-based \"flow\" models.\n",
        "control_flow_models = {\n",
        "    name: ControlGradientModel(model)\n",
        "    for name, model in zip(all_labels, models)\n",
        "}\n",
        "\n",
        "# 2. Combine the original learned flow models with the new control flow models.\n",
        "#    This creates a unified dictionary of all available manipulations.\n",
        "all_flow_models = {\n",
        "    **control_flow_models # Add age, gender, happy\n",
        "}\n",
        "\n",
        "# 3. Combine all regression models for metadata prediction.\n",
        "all_regression_models = {\n",
        "    **{name: model for name, model in zip(all_labels, models)}\n",
        "}\n",
        "\n",
        "# 4. Instantiate the backend with all models.\n",
        "backend = StyleGANFlowBackend(\n",
        "    generator=G,\n",
        "    flow_models=all_flow_models,\n",
        "    regression_models=all_regression_models,\n",
        "    trait_stats_df=df\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdP_QZ7SVs8Y"
      },
      "source": [
        "# Frontend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io7FnbYs026W"
      },
      "outputs": [],
      "source": [
        "%cd /Users/szymonlukiewicz/StudioProjects/psych_gen_app/build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMHxm-h3WKgs",
        "outputId": "9f927461-04f0-4d37-c736-1aa7052bee23"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, send_from_directory, jsonify, request\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "import time\n",
        "import json\n",
        "\n",
        "app = Flask(__name__, static_folder='web')\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return send_from_directory(app.static_folder, 'index.html')\n",
        "\n",
        "@app.route('/<path:path>')\n",
        "def static_files(path):\n",
        "    return send_from_directory(app.static_folder, path)\n",
        "\n",
        "def parse_config(conf):\n",
        "    \"\"\"\n",
        "    Zamienia przychodzƒÖcy JSON na s≈Çownik akceptowany przez StyleGANBackend.\n",
        "    ‚Ä¢ Dla ka≈ºdego wymiaru zwraca *listƒô py-float√≥w*, a nie tablicƒô NumPy.\n",
        "      Dziƒôki temu dalsze mno≈ºenie (float * Tensor) dzia≈Ça bez b≈Çƒôdu.\n",
        "    \"\"\"\n",
        "    if isinstance(conf, str):\n",
        "        conf = json.loads(conf)\n",
        "    print(json.dumps(conf, indent=2))\n",
        "\n",
        "    latents_from, latents_to = {\"both\": (0,18), \"color\": (9,18), \"shape\":(0,9)}[conf.pop(\"mode\", \"both\")]\n",
        "\n",
        "    dim1 = conf[\"manipulated_dimensions\"][0]\n",
        "    conf[\"manipulated_dimensions\"] = [dim1[\"name\"]]\n",
        "    # Note: np.linspace includes start and end points\n",
        "    conf[\"strengths\"] = [*np.linspace(-1*dim1[\"strength\"], dim1[\"strength\"], dim1[\"n_levels\"])]\n",
        "    print(conf[\"strengths\"])\n",
        "    conf[\"steps\"] = min(5,conf.pop(\"max_steps\", 5))\n",
        "    conf[\"latents_from\"] = latents_from\n",
        "    conf[\"latents_to\"] = latents_to\n",
        "    # dims = conf[\"manipulated_dimensions\"]\n",
        "    # conf[\"manipulated_dimensions\"] = [d[\"name\"] for d in dims]\n",
        "\n",
        "    # # ‚Üê kluczowa linia: tolist() zamienia ndarray na zwyk≈ÇƒÖ listƒô float√≥w\n",
        "    # if (len(dims) == 1):\n",
        "    #   conf[\"strengths\"] = [\n",
        "    #       *np.linspace(-1*dims[0][\"strength\"], dims[0][\"strength\"], dims[0][\"n_levels\"])\n",
        "    #   ]\n",
        "    # elif (len(dims) == 2):\n",
        "    #      conf[\"strengths\"] = [\n",
        "    #       *np.linspace(-1*dims[0][\"strength\"], dims[0][\"strength\"], dims[0][\"n_levels\"])\n",
        "    #       *np.linspace(-1*dims[1][\"strength\"], dims[1][\"strength\"], dims[1][\"n_levels\"])\n",
        "    #   ]\n",
        "    print(conf)\n",
        "    return conf\n",
        "\n",
        "@app.route('/images', methods=['POST'])\n",
        "def convert_images():\n",
        "      config = request.json\n",
        "      # config = parse_config(config)\n",
        "      config[\"num_faces\"]=1\n",
        "      config[\"return_metadata\"]=True\n",
        "\n",
        "      images_pil, metadata = backend(config)\n",
        "\n",
        "      image_array = images_pil[0]\n",
        "      converted_images = []\n",
        "      for i in range(len(image_array)):\n",
        "        for j in range(len(image_array[i])):\n",
        "          img = image_array[i][j]\n",
        "          if img is None:\n",
        "            continue\n",
        "          # Convert PIL Image to bytes\n",
        "          img_byte_arr = io.BytesIO()\n",
        "          img.save(img_byte_arr, format='PNG')\n",
        "          img_byte_arr = img_byte_arr.getvalue()\n",
        "\n",
        "          # Encode bytes to base64 string\n",
        "          image_array[i][j]=(base64.b64encode(img_byte_arr).decode('utf-8'))\n",
        "\n",
        "      print(image_array)\n",
        "      return image_array\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()#debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
